#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
LLM Comparator - ê³µì‹ í´ëŸ¬ìŠ¤í„°ë§ í¬í•¨ í•¨ìˆ˜í˜• ì •ë¦¬ ë²„ì „
"""

import os
import json
import sys
from datetime import datetime

import vertexai
from google.oauth2 import service_account

# ë§¨ ìœ„ import ë¶€ë¶„ì„ ì´ë ‡ê²Œ ìˆ˜ì •
from LLM_comparator.llm_comparator import comparison
from LLM_comparator.llm_comparator import llm_judge_runner
from LLM_comparator.llm_comparator import rationale_bullet_generator
from LLM_comparator.llm_comparator import rationale_cluster_generator
from LLM_comparator.llm_comparator import custom_model_helper

# ==========================
# ìƒìˆ˜ ì„¤ì • (í•„ìš”í•œ ë¶€ë¶„ë§Œ ìˆ˜ì •)
# ==========================
KEY_PATH = "../army22-12412f909096.json"  # ì„œë¹„ìŠ¤ ê³„ì • í‚¤ ê²½ë¡œ
PROJECT_ID = "army22"
LOCATION = "us-central1"

JUDGE_MODEL = "gemini-2.5-flash"
EMBEDDING_MODEL = "text-embedding-004"
MAX_OUTPUT_TOKENS = 2048  # í•„ìš”í•˜ë©´ custom_model_helperì—ì„œ ì‚¬ìš©

OUTPUT_DIR = "../llm_comparison_results"

os.environ["GOOGLE_APPLICATION_CREDENTIALS"] = os.path.abspath(KEY_PATH)
print("GOOGLE_APPLICATION_CREDENTIALS =", os.environ["GOOGLE_APPLICATION_CREDENTIALS"])

# ==========================
# 1. ì¸ì¦ & Vertex AI ì´ˆê¸°í™”
# ==========================
def authenticate_vertex_ai(key_path: str) -> service_account.Credentials:
    """ì„œë¹„ìŠ¤ ê³„ì • í‚¤ë¡œ Vertex AI ì¸ì¦."""
    print("\n[1ë‹¨ê³„] Vertex AI ì¸ì¦")
    if not os.path.exists(key_path):
        raise FileNotFoundError(f"í‚¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ: {key_path}")
    print(f"í‚¤ íŒŒì¼ í™•ì¸ë¨: {key_path}")
    credentials = service_account.Credentials.from_service_account_file(
        key_path,
        scopes=["https://www.googleapis.com/auth/cloud-platform"],
    )
    print("  âœ“ ì¸ì¦ ì™„ë£Œ")
    return credentials


def init_vertex_ai(project: str, location: str, credentials) -> None:

    print("\n[2ë‹¨ê³„] Vertex AI ì´ˆê¸°í™”")
    vertexai.init(
        project=project,
        location=location,
        credentials=credentials,
    )
    print("  âœ“ ì´ˆê¸°í™” ì™„ë£Œ")


# ==========================
# 2. ë°ì´í„° ë¡œë“œ ë° ë³€í™˜
# ==========================
def load_llm_outputs(llm1_file: str, llm2_file: str):
    """LLM1/LLM2ì˜ ì¶œë ¥ JSON íŒŒì¼ì„ ë¡œë“œ."""
    print("\n[3ë‹¨ê³„] ë°ì´í„° ë¡œë“œ")

    with open(llm1_file, "r", encoding="utf-8") as f:
        llm1_data = json.load(f)

    with open(llm2_file, "r", encoding="utf-8") as f:
        llm2_data = json.load(f)

    print(f"  âœ“ LLM1 ì‘ë‹µ: {len(llm1_data['examples'])}ê°œ")
    print(f"  âœ“ LLM2 ì‘ë‹µ: {len(llm2_data['examples'])}ê°œ")

    model_a_name = llm1_data["metadata"]["model_name"]
    model_b_name = llm2_data["metadata"]["model_name"]

    print(f"  âœ“ Model A: {model_a_name}")
    print(f"  âœ“ Model B: {model_b_name}")

    return llm1_data, llm2_data, model_a_name, model_b_name


def build_comparator_inputs(llm1_data, llm2_data):
    """LLM Comparatorì—ì„œ ì‚¬ìš©í•˜ëŠ” inputs í¬ë§·ìœ¼ë¡œ ë³€í™˜."""
    print("\n[4ë‹¨ê³„] ë°ì´í„° ë³€í™˜")
    inputs = []
    for item1, item2 in zip(llm1_data["examples"], llm2_data["examples"]):
        inputs.append(
            {
                "prompt": item1["prompt"],
                "response_a": item1["response"],
                "response_b": item2["response"],
            }
        )

    print(f"  âœ“ {len(inputs)}ê°œ ì§ˆë¬¸ ì¤€ë¹„ ì™„ë£Œ")
    return inputs

def transform_data(merge_data):
    inputs = []
    for item in merge_data['examples']:
        inputs.append(
            {
                "prompt": item["input_text"],
                "response_a": item["output_text_a"],
                "response_b": item["output_text_b"],
            }
        )

    print(f"  âœ“ {len(inputs)}ê°œ ì§ˆë¬¸ ì¤€ë¹„ ì™„ë£Œ")
    return inputs

# ==========================
# 3. ëª¨ë¸ í—¬í¼ & ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
# ==========================
def init_model_helpers(judge_model: str, embedding_model: str):
    """Judge/Embeddingìš© Vertex ëª¨ë¸ í—¬í¼ ì´ˆê¸°í™”."""
    print("\n[5ë‹¨ê³„] ëª¨ë¸ í—¬í¼ ì´ˆê¸°í™”")
    print(f"  - Judge Model: {judge_model}")
    print(f"  - Embedding Model: {embedding_model}")
    print(f"  - Max Output Tokens: {MAX_OUTPUT_TOKENS}")

    generator = custom_model_helper.VertexGenerationModelHelper(judge_model)
    embedder = custom_model_helper.VertexEmbeddingModelHelper(embedding_model)
    print("  âœ“ ëª¨ë¸ í—¬í¼ ì¤€ë¹„ ì™„ë£Œ")

    return generator, embedder

def init_comparator_components(generator, embedder):
    """LLM Comparatorì˜ Judge/Bulletizer/Clusterer ì´ˆê¸°í™”."""
    print("\n[6ë‹¨ê³„] LLM Comparator ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”")
    judge = llm_judge_runner.LLMJudgeRunner(generator)
    bulletizer = rationale_bullet_generator.RationaleBulletGenerator(generator)
    clusterer = rationale_cluster_generator.RationaleClusterGenerator(
        generator, embedder
    )
    print("  âœ“ Judge, Bulletizer, Clusterer ì¤€ë¹„ ì™„ë£Œ")
    return judge, bulletizer, clusterer

# ==========================
# 4. LLM Comparator ì‹¤í–‰
# ==========================
def run_llm_comparator(inputs, judge, bulletizer, clusterer, model_a_name, model_b_name):
    """LLM Comparatorë¥¼ ì‹¤í–‰í•˜ê³  ê²°ê³¼ì™€ ì‹¤í–‰ ì‹œê°„ì„ ë°˜í™˜."""
    print("\n[7ë‹¨ê³„] LLM Comparator ì‹¤í–‰ (ê³µì‹ í´ëŸ¬ìŠ¤í„°ë§ í¬í•¨)")
    print(f"  - ì´ ì§ˆë¬¸ ìˆ˜: {len(inputs)}ê°œ")
    print(f"\n  â° ì‹œìž‘ ì‹œê°„: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n")

    start_time = datetime.now()

    try:
        comparison_result = comparison.run(
            inputs,
            judge,
            bulletizer,
            clusterer,
            model_names=(model_a_name, model_b_name),
        )
    except Exception as e:
        print(f"\n  âŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback

        traceback.print_exc()
        raise

    end_time = datetime.now()
    elapsed_time = (end_time - start_time).total_seconds()

    print(f"\n  âœ“ LLM Comparator ì‹¤í–‰ ì™„ë£Œ")
    print(f"  â° ì¢…ë£Œ ì‹œê°„: {end_time.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"  â±ï¸  ì†Œìš” ì‹œê°„: {elapsed_time / 60:.1f}ë¶„")

    return comparison_result, elapsed_time

# ==========================
# 5. ê²°ê³¼ ì €ìž¥
# ==========================
def save_comparison_result(comparison_result, output_dir: str) -> str:
    """ë¹„êµ ê²°ê³¼ë¥¼ JSON íŒŒì¼ë¡œ ì €ìž¥í•˜ê³  ê²½ë¡œë¥¼ ë°˜í™˜."""
    print("\n[8ë‹¨ê³„] ê²°ê³¼ ì €ìž¥")
    os.makedirs(output_dir, exist_ok=True)

    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"{output_dir}/llm_comparator_auto_viewer_{timestamp}.json"

    comparison.write(comparison_result, output_file)
    print(f"  âœ“ ê²°ê³¼ ì €ìž¥: {output_file}")
    return output_file

# ==========================
# 6. í†µê³„ ì¶œë ¥
# ==========================
def print_overall_stats(comparison_result, judge_model, model_a_name, model_b_name):
    """ìŠ¹/íŒ¨/ë™ì  ë° ê¸°ë³¸ í†µê³„ ì¶œë ¥."""
    print("\n" + "=" * 80)
    print("í‰ê°€ ì™„ë£Œ!")
    print("=" * 80)

    examples = comparison_result["examples"]
    scores = [ex["score"] for ex in examples]

    a_wins = sum(1 for s in scores if s > 0)
    b_wins = sum(1 for s in scores if s < 0)
    ties = sum(1 for s in scores if s == 0)

    print(f"\nðŸ“Š ê²°ê³¼ ìš”ì•½:")
    print(f"  â€¢ ì´ í‰ê°€ ìŒ: {len(examples)}ê°œ")
    print(f"  â€¢ Judge ëª¨ë¸: {judge_model}")
    print(f"  â€¢ Model A ({model_a_name}): {a_wins}ìŠ¹ ({a_wins / len(examples) * 100:.1f}%)")
    print(f"  â€¢ Model B ({model_b_name}): {b_wins}ìŠ¹ ({b_wins / len(examples) * 100:.1f}%)")
    print(f"  â€¢ ë™ì : {ties}ê°œ ({ties / len(examples) * 100:.1f}%)")
    print(f"  â€¢ í‰ê·  ì ìˆ˜ ì°¨ì´: {sum(scores) / len(scores):.3f}")


def print_rationale_stats(comparison_result):
    """Rationale ê´€ë ¨ í†µê³„ ì¶œë ¥."""
    examples = comparison_result["examples"]

    rationale_count = 0
    total_ratings = 0
    for ex in examples:
        individual_scores = ex.get("individual_rater_scores", [])
        total_ratings += len(individual_scores)
        for score_item in individual_scores:
            if isinstance(score_item, dict) and score_item.get("rationale"):
                rationale_count += 1

    print(f"\nðŸ“ Rationale í†µê³„:")
    print(f"  â€¢ ì´ í‰ê°€ íšŸìˆ˜: {total_ratings}íšŒ")
    print(f"  â€¢ Rationale í¬í•¨: {rationale_count}íšŒ")
    if total_ratings > 0:
        print(f"  â€¢ Rationale ë¹„ìœ¨: {rationale_count / total_ratings * 100:.1f}%")


def print_cluster_stats(comparison_result):
    """í´ëŸ¬ìŠ¤í„°ë§ í†µê³„ ì¶œë ¥."""
    clusters = comparison_result.get("rationale_clusters", [])
    if not clusters:
        print(f"\nâš ï¸  í´ëŸ¬ìŠ¤í„°ë§ ì •ë³´ ì—†ìŒ")
        return

    print(f"\nðŸ” í´ëŸ¬ìŠ¤í„°ë§ í†µê³„:")
    print(f"  â€¢ í´ëŸ¬ìŠ¤í„° ìˆ˜: {len(clusters)}ê°œ")
    print(f"\n  í´ëŸ¬ìŠ¤í„° ëª©ë¡:")
    for i, cluster in enumerate(clusters, 1):
        title = cluster.get("title", f"Cluster {i}")
        print(f"    {i}. {title}")


def print_top_examples(comparison_result, top_k: int = 5):
    """ìƒìœ„ top_kê°œ ì˜ˆì‹œ ì¶œë ¥."""
    examples = comparison_result["examples"]
    print(f"\nðŸ“‹ ìƒìœ„ {top_k}ê°œ ì§ˆë¬¸ ê²°ê³¼:")
    for i, ex in enumerate(examples[:top_k], 1):
        text_preview = ex.get("input_text", "")[:60]
        print(f"\n  [{i}] {text_preview}...")
        score = ex["score"]
        print(f"      ì ìˆ˜: {score:.2f}", end="")
        if score > 0.5:
            print(" â†’ Model A ìŠ¹ë¦¬")
        elif score < -0.5:
            print(" â†’ Model B ìŠ¹ë¦¬")
        else:
            print(" â†’ ë¹„ìŠ·í•¨")

# ==========================
# 7. VSCode ì›¹ UI ìžë™ ì‹¤í–‰
# ==========================
def open_vscode_viewer(output_file: str):
    """VSCode í™˜ê²½ì—ì„œ LLM Comparator ì›¹ UIë¥¼ ìžë™ ì‹¤í–‰."""
    print("\n" + "=" * 80)
    print("[10ë‹¨ê³„] VSCodeì—ì„œ ì›¹ UI ìžë™ ì‹¤í–‰")
    print("=" * 80)

    try:
        # íŒ¨ì¹˜ëœ comparison.pyì˜ show_in_vscode() ì‚¬ìš©
        comparison.show_in_vscode(output_file)
    except KeyboardInterrupt:
        print("\n\nâœ… ì‚¬ìš©ìžê°€ ì„œë²„ë¥¼ ì¢…ë£Œí–ˆìŠµë‹ˆë‹¤.")
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        print(f"\nìˆ˜ë™ìœ¼ë¡œ í™•ì¸í•˜ë ¤ë©´:")
        print("  1. https://pair-code.github.io/llm-comparator/ ì ‘ì†")
        print("  2. 'Load data' ë²„íŠ¼ í´ë¦­")
        print(f"  3. {output_file} ì—…ë¡œë“œ")

# ==========================
# main
# ==========================
def main():
    print("=" * 80)
    print("LLM Comparator - ê³µì‹ í´ëŸ¬ìŠ¤í„°ë§ í¬í•¨ ì™„ì „ ë²„ì „")
    print("=" * 80)

    # 1) ì¸ì¦ ë° ì´ˆê¸°í™”
    credentials = authenticate_vertex_ai(KEY_PATH)
    init_vertex_ai(PROJECT_ID, LOCATION, credentials)

    with open('../llm_comparator_input.json', "r", encoding="utf-8") as f:
        merge_data = json.load(f)

    model_a_name = merge_data["metadata"]["A_model_name"]
    model_b_name = merge_data["metadata"]["B_model_name"] 
    inputs = transform_data(merge_data)

    # 3) ëª¨ë¸ í—¬í¼ ë° ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
    generator, embedder = init_model_helpers(JUDGE_MODEL, EMBEDDING_MODEL)
    judge, bulletizer, clusterer = init_comparator_components(generator, embedder)

    # 4) LLM Comparator ì‹¤í–‰
    comparison_result, _ = run_llm_comparator(
        inputs, judge, bulletizer, clusterer, model_a_name, model_b_name
    )

    # 5) ê²°ê³¼ ì €ìž¥
    output_file = save_comparison_result(comparison_result, OUTPUT_DIR)

    # 6) í†µê³„ ì¶œë ¥
    print_overall_stats(comparison_result, JUDGE_MODEL, model_a_name, model_b_name)
    print_rationale_stats(comparison_result)
    print_cluster_stats(comparison_result)
    print_top_examples(comparison_result, top_k=5)

    print(f"\nðŸ“ ì¶œë ¥ íŒŒì¼:")
    print(f"  {output_file}")

    # 7) VSCode ì›¹ UI ì‹¤í–‰
    open_vscode_viewer(output_file)

    print("\n" + "=" * 80)
    print("âœ… ëª¨ë“  ìž‘ì—… ì™„ë£Œ!")
    print("=" * 80)


if __name__ == "__main__":
    main()
